{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9011497,"sourceType":"datasetVersion","datasetId":5429137},{"sourceId":9040544,"sourceType":"datasetVersion","datasetId":5450230}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alexander Xu (2024)","metadata":{}},{"cell_type":"code","source":"# Import libraries\n\nimport os\nimport shutil\nimport math\nimport random\nimport cv2\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom skimage import io, color, exposure, filters, morphology\nfrom skimage.feature import peak_local_max\nfrom skimage.segmentation import watershed\n\nfrom scipy import ndimage as ndi\nfrom scipy.ndimage import label, generate_binary_structure\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import SGD\n\n!pip install albumentations\nimport albumentations as A\n\nrandom.seed(7)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Utility functions**","metadata":{}},{"cell_type":"code","source":"# Augment all images in a folder\ndef augment_folder(input_image_folder,input_mask_folder,output_image_folder,output_mask_folder):\n    os.mkdir(output_image_folder, exist_ok=True)\n    os.mkdir(output_mask_folder, exist_ok=True)\n    \n    # Augmentation settings via Albumentations libraryüêêüêê\n    transform = A.Compose([\n            A.CLAHE(p=0.2,clip_limit=6),\n            A.Blur(p=0.2,blur_limit=10),\n            A.ColorJitter(brightness=(0.5, 1), contrast=(0.5, 1), saturation=(0.5, 1), hue=(-0.5, 0.5), p=0.2),\n            A.Sharpen(alpha=(0.2, 1.0), lightness=(0.2, 1.0), p=0.2),\n            A.RGBShift(r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20), p=0.2),\n            A.Defocus(radius=(1, 20),alias_blur=(0.1, 1.0),p=0.2),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.2),\n            A.VerticalFlip(p=0.2),\n            A.RandomRotate90(p=0.2),\n            A.RandomGridShuffle(grid=(3, 3), p=0.2),\n            A.Flip(p=0.2),\n            A.HorizontalFlip(p=0.2),\n            A.ToGray(p=0.2),\n            A.ChannelShuffle(p=0.2),\n            A.ChannelDropout(channel_drop_range=(1, 1), fill_value=0, p=0.2),\n            A.MultiplicativeNoise(multiplier=[0.5, 1.5], elementwise=True, p=0.2),\n            A.OneOf([A.OpticalDistortion(p=0.2),A.GridDistortion(p=0.2)], p=0.2),\n            A.PixelDropout(dropout_prob=0.02,drop_value=0,p=0.2)])\n    \n    for filename in tqdm(os.listdir(input_image_folder)):\n        image = cv2.imread(os.path.join(input_image_folder, filename))\n        mask = cv2.imread(os.path.join(input_mask_folder, filename))\n        \n        # copy original images\n        cv2.imwrite(os.path.join(output_image_folder,f'aug_{filename}'),image)\n        cv2.imwrite(os.path.join(output_mask_folder,f'aug_{filename}'),mask)\n        \n        for i in range(AUG_REPS):\n            augmented = transform(image=image, mask=mask)        \n            cv2.imwrite(os.path.join(output_image_folder,f'aug{i}_{filename}'),augmented['image'])\n            cv2.imwrite(os.path.join(output_mask_folder,f'aug{i}_{filename}'),augmented['mask'])\n\n# Tile all images in a folder\ndef tile_folder(image_folder,output_folder):\n    os.mkdir(output_folder, exist_ok=True)\n    \n    for filename in tqdm(os.listdir(image_folder)):\n        image_path = os.path.join(image_folder, filename)\n        image = cv2.imread(image_path)\n        img_shape = image.shape\n\n        for i in range(img_shape[0] // TILE_SIZE):\n            for j in range(img_shape[1] // TILE_SIZE):\n                # Crop tile size section of image using row and col\n                tiled_img = image[TILE_SIZE * i:min(TILE_SIZE * (i + 1), img_shape[0]), TILE_SIZE * j:min(TILE_SIZE * (j + 1), img_shape[1])]\n                cv2.imwrite(os.path.join(output_folder, f'{i}_{j}_{filename}'), tiled_img)\n\n# Load all training images and masks\ndef load_images_and_masks(image_folder,mask_folder):\n    pos_images, pos_masks, neg_images, neg_masks = [],[],[],[]\n    \n    for filename in os.listdir(image_folder):\n        image = cv2.imread(os.path.join(image_folder, filename))\n        mask = cv2.imread(os.path.join(mask_folder, filename))\n        \n        if np.all(mask == 0):\n            neg_images.append(image)\n            neg_masks.append(mask)\n        else:\n            pos_images.append(image)\n            pos_masks.append(mask)\n    \n    return pos_images, pos_masks, neg_images, neg_masks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Train Dataset Generation Parameters\nTILE_SIZE = 64 # Divisble by 2^4 for U-Net architecture\nAUG_REPS = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_folder('/data/images','/tiled_images')\ntile_folder('/data/masks','/tiled_masks')\naugment_folder('/tiled_images','/tiled_masks','/aug_images','/aug_masks')\n\npos_images, pos_masks, neg_images, neg_masks = load_images_and_masks('/aug_images','/aug_masks')\nmatched_indices = random.sample(range(len(neg_images)), len(pos_images)) # Equal number of 'positive' (tiles with droplets) as 'negatives' (tiles without droplets)\nlipid_images = pos_images + [neg_images[i] for i in matched_indices]\nlipid_masks = pos_masks + [neg_masks[i] for i in matched_indices]\n\n# both train and val are augmented due to dataset size\nX = np.zeros((len(lipid_images), TILE_SIZE, TILE_SIZE, 3))\nY = np.zeros((len(lipid_masks), TILE_SIZE, TILE_SIZE, 1))\n\nfor idx,image in enumerate(lipid_images):\n    X[idx]=image\n\nfor idx,mask in enumerate(lipid_masks):\n    Y[idx]=mask[:, :, :1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**U-Net Model training**","metadata":{}},{"cell_type":"code","source":"def downsample_block(n_filters,prev_layer):\n    c1 = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(prev_layer)\n    c1 = tf.keras.layers.Dropout(0.1)(c1)\n    c1 = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu',kernel_initializer='he_normal', padding='same')(c1)\n    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n    return c1,p1\n\ndef upsample_block(n_filters,prev_layer,skip_layer):\n    u1 = tf.keras.layers.Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(prev_layer)\n    u1 = tf.keras.layers.concatenate([u1, skip_layer])\n    c1 = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u1)\n    c1 = tf.keras.layers.Dropout(0.1)(c1)\n    c1 = tf.keras.layers.Conv2D(n_filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    return c1,u1\n\n# Creates U-Net model\ndef build_unet_model():\n    # Input layer\n    inputs = tf.keras.layers.Input((TILE_SIZE, TILE_SIZE, 3))\n    \n    # Normalization\n    s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n\n    # Contraction Path\n    c1,p1=downsample_block(16,s)\n    c2,p2=downsample_block(32,p1)\n    c3,p3=downsample_block(64,p2)\n    c4,p4=downsample_block(128,p3)\n    \n    # Bottleneck layer\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = tf.keras.layers.Dropout(0.2)(c5)\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n    # Expansion Path\n    c6,u6=upsample_block(128,c5,c4)\n    c7,u7=upsample_block(64,c6,c3)\n    c8,u8=upsample_block(32,c7,c2)\n    c9,u9=upsample_block(16,c8,c1)\n\n    # Output layer for density map\n    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n    \n    return inputs,outputs\n\n# Custom loss function\ndef FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n    #flatten label and prediction tensors\n    inputs = Flatten()(inputs)\n    targets = Flatten()(targets)\n\n    TP = K.sum((inputs * targets))\n    FP = K.sum(((1-targets) * inputs))\n    FN = K.sum((targets * (1-inputs)))\n\n    Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n    FocalTversky = K.pow((1 - Tversky), gamma)\n\n    return FocalTversky","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Loss Parameters\nALPHA = 0.5\nBETA = 0.5\nGAMMA = 1\n\n# U-Net Parameters\nEPOCHS = 40\nBATCH_SIZE = 32\nVAL_SPLIT = 0.2\nLEARNING_RATE = 5e-4\nPATIENCE = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model initialization\ninputs, outputs=build_unet_model()\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),loss='binary_crossentropy',metrics=[\"accuracy\"])\n\n# checkpointer = tf.keras.callbacks.ModelCheckpoint(' /lipid_model.keras',verbose=1, save_best_only=True)\ncallbacks = [tf.keras.callbacks.TensorBoard(log_dir='logs'),tf.keras.callbacks.EarlyStopping(patience=PATIENCE, monitor='val_loss')]\n\n# Model train\nhistory = model.fit(X, Y, validation_split=VAL_SPLIT, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/val accuracy plot\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n\n# Train/val loss plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Density map postprocessing**","metadata":{}},{"cell_type":"code","source":"def normalize_image(image):\n    return tf.cast(image, tf.float32) / 255.0\n\ndef to_binary(image):\n    _,binary = cv2.threshold(image, 0.2, 1, cv2.THRESH_BINARY)\n    return binary.astype(np.float32)\n\n# Tile input image for inference\ndef tile_image(image):\n    tiles = []\n    img_shape = image.shape\n    \n    for i in range(img_shape[0] // TILE_SIZE):\n        for j in range(img_shape[1] // TILE_SIZE):\n            tiled_img = image[\n                TILE_SIZE * i:min(TILE_SIZE * (i + 1), img_shape[0]),\n                TILE_SIZE * j:min(TILE_SIZE * (j + 1), img_shape[1])\n            ]\n            tiles.append(tiled_img)\n    \n    return tiles\n\n# Recombine tiles\ndef stitch_image(tiles,image):\n    img_shape = image.shape\n    rows = [\n        np.concatenate(tiles[row_i * (img_shape[1] // TILE_SIZE):(row_i + 1) * (img_shape[1] // TILE_SIZE)], axis=1)\n        for row_i in range(img_shape[0] // TILE_SIZE)\n    ]\n    \n    return np.concatenate(rows,axis=0)\n\n# Recombine centroid predictions\ndef stitch_centroids(centroids,image):\n    img_shape = image.shape\n    res = [(\n            tile_centroid[0] + (i // (img_shape[1] // TILE_SIZE)) * TILE_SIZE,\n            tile_centroid[1] + (i % (img_shape[1] // TILE_SIZE)) * TILE_SIZE)\n        for i, tile_centroids in enumerate(centroids)\n        for tile_centroid in tile_centroids]\n    \n    return res\n\n# Extract cell background\ndef extract_stain(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = exposure.equalize_adapthist(gray, clip_limit=0.03)\n    gray = filters.rank.mean_bilateral(gray, morphology.disk(30))\n    gray = cv2.fastNlMeansDenoising(gray.astype(np.uint8), h=10)\n    gray = cv2.bitwise_not(gray)\n    gray = filters.unsharp_mask(gray, radius=1, amount=1)\n    \n    binary = gray > filters.threshold_otsu(gray)   \n    binary = morphology.dilation(binary, morphology.disk(1))\n    binary = morphology.remove_small_objects(binary.astype(bool), min_size=200)\n    \n    return binary.astype(np.float32)\n\ndef detect_centroids(feature_map):\n    labeled_array, num_features = label(feature_map)\n    centers = ndi.center_of_mass(feature_map, labeled_array, range(1, num_features+1))\n    return centers\n\n# Exclude centroids not present on stains\ndef filter_centroids(centroids,stain):\n    return [centroid for centroid in centroids if stain[int(centroid[0])][int(centroid[1])]]\n\n# Main function to make predictions on entire folder\ndef predict_folder(image_folder,model,verbose=False):\n    res = {}\n    for filename in os.listdir(image_folder):\n        n_features = predict_image(os.path.join(image_folder, filename), model, verbose = verbose)\n        res[filename] = n_features\n\n        if verbose:\n            print(f'{filename}: {n_features}')\n    \n    return res\n        \n# Make prediction on a single file\ndef predict_image(image_path,model,verbose=False):\n    image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n    \n    # Image pre processing\n    working_image = normalize_image(image)\n    stain = extract_stain(image)\n    tiles = tile_image(working_image)\n    \n    # U-Net prediction\n    preds = model.predict(np.array(tiles), verbose=False)\n#     pred_image=stitch_image(preds,image)    # Predictions can be visualized if needed\n    binaries = [to_binary(pred) for pred in preds]\n    \n    # Centroid identification\n    centroids = [detect_centroids(binary) for binary in binaries]\n    scaled_centroids = stitch_centroids(centroids,image)\n    all_centroids = filter_centroids(scaled_centroids,stain)\n\n    # Debug\n    if verbose:\n        grid_color = [0,0,0]\n        image[:,::TILE_SIZE,:] = grid_color\n        image[::TILE_SIZE,:,:] = grid_color\n        \n        plt.figure(figsize=(16,10))\n        plt.grid(False)\n        plt.axis('off')\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n        \n        x, y = np.array(all_centroids).T\n        plt.scatter(y, x, s = 10, c = 'w')\n        plt.show()\n            \n    return len(all_centroids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeline testing**","metadata":{}},{"cell_type":"code","source":"inputs,outputs=build_unet_model()\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel.load_weights('/unetmodel.weights.h5')\n\npredict_folder('/test/images',model,verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}